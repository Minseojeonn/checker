2024-11-15 03:29:43.790 | INFO     | __main__:main:41 - The main procedure has started with the following parameters:
2024-11-15 03:29:43.834 | INFO     | utils:log_param:32 - model               :                                          LightGcn
2024-11-15 03:29:43.834 | INFO     | utils:log_param:32 - seed                :                                                 1
2024-11-15 03:29:43.834 | INFO     | utils:log_param:32 - device              :                                            cuda:0
2024-11-15 03:29:43.834 | INFO     | utils:log_param:32 - dataset             :                                             ml-1m
2024-11-15 03:29:43.835 | INFO     | utils:log_param:32 - split_ratio         :                                   [0.8, 0.1, 0.1]
2024-11-15 03:29:43.835 | INFO     | utils:log_param:32 - shuffle             :                                              True
2024-11-15 03:29:43.835 | INFO     | dataloader.template:load_dataset:55 - Preprocessed data exists
/home/minseo/back_up_3/checker/src/dataloader/template.py:184: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/torch/csrc/utils/tensor_new.cpp:618.)
  self.Graph = torch.sparse.FloatTensor(index, data, torch.Size([self.num_users+self.num_items, self.num_users+self.num_items]))
2024-11-15 03:29:45.615 | INFO     | __main__:main:63 - Training the model has begun with the following hyperparameters:
2024-11-15 03:29:45.615 | INFO     | utils:log_param:32 - epochs              :                                               100
2024-11-15 03:29:45.615 | INFO     | utils:log_param:32 - learning_rate       :                                             0.001
2024-11-15 03:29:45.615 | INFO     | utils:log_param:32 - in_dim              :                                                20
2024-11-15 03:29:45.615 | INFO     | utils:log_param:32 - out_dim             :                                                20
2024-11-15 03:29:45.615 | INFO     | utils:log_param:32 - layer_num           :                                                 3
2024-11-15 03:29:45.615 | INFO     | utils:log_param:32 - batch_size          :                                            300000
2024-11-15 03:29:45.615 | INFO     | utils:log_param:32 - device              :                                            cuda:0
2024-11-15 03:29:45.615 | INFO     | utils:log_param:32 - Ks                  :                                   [5, 10, 15, 20]
0
0 833
1 833
2 833
3 833
4 833
5 833
6 833
7 833
8 833
9 833
10 833
11 833
12 833
13 833
14 833
15 833
16 833
17 833
18 833
19 833
20 833
21 833
22 833
23 833
24 833
25 833
26 833
27 833
28 833
29 833
30 833
31 833
32 833
33 833
34 833
35 833
36 833
37 833
38 833
39 833
40 833
41 833
42 833
43 833
44 833
45 833
46 833
47 833
48 833
49 833
50 833
51 833
52 833
53 833
54 833
55 833
56 833
57 833
58 833
59 833
60 833
61 833
62 833
63 833
64 833
65 833
66 833
67 833
68 833
69 833
70 833
71 833
72 833
73 833
74 833
75 833
76 833
77 833
78 833
79 833
80 833
81 833
82 833
83 833
84 833
85 833
86 833
87 833
88 833
89 833
90 833
91 833
92 833
93 833
94 833
95 833
96 833
97 833
98 833
99 833
100 833
101 833
102 833
103 833
104 833
105 833
106 833
107 833
108 833
109 833
110 833
111 833
112 833
113 833
114 833
115 833
116 833
117 833
118 833
119 833
120 833
121 833
122 833
123 833
124 833
125 833
126 833
127 833
128 833
129 833
130 833
131 833
132 833
133 833
134 833
135 833
136 833
137 833
138 833
139 833
140 833
141 833
142 833
143 833
144 833
145 833
146 833
147 833
148 833
149 833
150 833
151 833
152 833
153 833
154 833
155 833
156 833
157 833
158 833
159 833
160 833
161 833
162 833
163 833
164 833
165 833
166 833
167 833
168 833
169 833
170 833
171 833
172 833
173 833
174 833
175 833
176 833
177 833
178 833
179 833
180 833
181 833
182 833
183 833
184 833
185 833
186 833
187 833
188 833
189 833
190 833
191 833
192 833
193 833
194 833
195 833
196 833
197 833
198 833
199 833
200 833
201 833
202 833
203 833
204 833
205 833
206 833
207 833
208 833
209 833
210 833
211 833
212 833
213 833
214 833
215 833
216 833
217 833
218 833
219 833
220 833
221 833
222 833
223 833
224 833
225 833
226 833
227 833
228 833
229 833
230 833
231 833
232 833
233 833
234 833
235 833
236 833
237 833
238 833
239 833
240 833
241 833
242 833
243 833
244 833
245 833
246 833
247 833
248 833
249 833
250 833
251 833
252 833
253 833
254 833
255 833
256 833
257 833
258 833
259 833
260 833
261 833
262 833
263 833
264 833
265 833
266 833
267 833
268 833
269 833
270 833
271 833
272 833
273 833
274 833
275 833
276 833
277 833
278 833
279 833
280 833
281 833
282 833
283 833
284 833
285 833
286 833
287 833
288 833
289 833
290 833
291 833
292 833
293 833
294 833
295 833
296 833
297 833
298 833
299 833
300 833
301 833
302 833
303 833
304 833
305 833
306 833
307 833
308 833
309 833
310 833
311 833
312 833
313 833
314 833
315 833
316 833
317 833
318 833
319 833
320 833
321 833
322 833
323 833
324 833
325 833
326 833
327 833
328 833
329 833
330 833
331 833
332 833
333 833
334 833
335 833
336 833
337 833
338 833
339 833
340 833
341 833
342 833
343 833
344 833
345 833
346 833
347 833
348 833
349 833
350 833
351 833
352 833
353 833
354 833
355 833
356 833
357 833
358 833
359 833
360 833
361 833
362 833
363 833
364 833
365 833
366 833
367 833
368 833
369 833
370 833
371 833
372 833
373 833
374 833
375 833
376 833
377 833
378 833
379 833
380 833
381 833
382 833
383 833
384 833
385 833
386 833
387 833
388 833
389 833
390 833
391 833
392 833
393 833
394 833
395 833
396 833
397 833
398 833
399 833
400 833
401 833
402 833
403 833
404 833
405 833
406 833
407 833
408 833
409 833
410 833
411 833
412 833
413 833
414 833
415 833
416 833
417 833
418 833
419 833
420 833
421 833
422 833
423 833
424 833
425 833
426 833
427 833
428 833
429 833
430 833
431 833
432 833
433 833
434 833
435 833
436 833
437 833
438 833
439 833
440 833
441 833
442 833
443 833
444 833
445 833
446 833
447 833
448 833
449 833
450 833
451 833
452 833
453 833
454 833
455 833
456 833
457 833
458 833
459 833
460 833
461 833
462 833
463 833
464 833
465 833
466 833
467 833
468 833
469 833
470 833
471 833
472 833
473 833
474 833
475 833
476 833
477 833
478 833
479 833
480 833
481 833
482 833
483 833
484 833
485 833
486 833
487 833
488 833
489 833
490 833
491 833
492 833
493 833
494 833
495 833
496 833
497 833
498 833
499 833
500 833
501 833
502 833
503 833
504 833
505 833
506 833
507 833
508 833
509 833
510 833
511 833
512 833
513 833
514 833
515 833
516 833
517 833
518 833
519 833
520 833
521 833
522 833
523 833
524 833
525 833
526 833
527 833
528 833
529 833
530 833
531 833
532 833
533 833
534 833
535 833
536 833
537 833
538 833
539 833
540 833
541 833
542 833
543 833
544 833
545 833
546 833
547 833
548 833
549 833
550 833
551 833
552 833
553 833
554 833
555 833
556 833
557 833
558 833
559 833
560 833
561 833
562 833
563 833
564 833
565 833
566 833
567 833
568 833
569 833
570 833
571 833
572 833
573 833
574 833
575 833
576 833
577 833
578 833
579 833
580 833
581 833
582 833
583 833
584 833
585 833
586 833
587 833
588 833
589 833
590 833
591 833
592 833
593 833
594 833
595 833
596 833
597 833
598 833
599 833
600 833
601 833
602 833
603 833
604 833
605 833
606 833
607 833
608 833
609 833
610 833
611 833
612 833
613 833
614 833
615 833
616 833
617 833
618 833
619 833
620 833
621 833
622 833
623 833
624 833
625 833
626 833
627 833
628 833
629 833
630 833
631 833
632 833
633 833
634 833
635 833
636 833
637 833
638 833
639 833
640 833
641 833
642 833
643 833
644 833
645 833
646 833
647 833
648 833
649 833
650 833
651 833
652 833
653 833
654 833
655 833
656 833
657 833
658 833
659 833
660 833
661 833
662 833
663 833
664 833
665 833
666 833
667 833
668 833
669 833
670 833
671 833
672 833
673 833
674 833
675 833
676 833
677 833
678 833
679 833
680 833
681 833
682 833
683 833
684 833
685 833
686 833
687 833
688 833
689 833
690 833
691 833
692 833
693 833
694 833
695 833
696 833
697 833
698 833
699 833
700 833
701 833
702 833
703 833
704 833
705 833
706 833
707 833
708 833
709 833
710 833
711 833
712 833
713 833
714 833
715 833
716 833
717 833
718 833
719 833
720 833
721 833
722 833
723 833
724 833
725 833
726 833
727 833
728 833
729 833
730 833
731 833
732 833
733 833
734 833
735 833
736 833
737 833
738 833
739 833
740 833
741 833
742 833
743 833
744 833
745 833
746 833
747 833
748 833
749 833
750 833
751 833
752 833
753 833
754 833
755 833
756 833
757 833
758 833
759 833
760 833
761 833
762 833
763 833
764 833
765 833
766 833
767 833
768 833
769 833
770 833
771 833
772 833
773 833
774 833
775 833
776 833
777 833
778 833
779 833
780 833
781 833
782 833
783 833
784 833
785 833
786 833
787 833
788 833
789 833
790 833
791 833
792 833
793 833
794 833
795 833
796 833
797 833
798 833
799 833
800 833
801 833
802 833
803 833
804 833
805 833
806 833
807 833
808 833
809 833
810 833
811 833
812 833
813 833
814 833
815 833
816 833
817 833
818 833
819 833
820 833
821 833
822 833
823 833
824 833
825 833
826 833
827 833
828 833
829 833
830 833
831 833
832 833
833 833
> /home/minseo/back_up_3/checker/src/models/LightGCN/LightgcnTrainer.py(106)train_with_hyper_param()
-> print(f"test: epoch: {epoch}, recall: {recall}, precision: {precision}, ndcg: {ndcg}")
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/minseo/back_up_3/checker/src/main.py", line 92, in <module>
    sys.exit(fire.Fire(main))
             ^^^^^^^^^^^^^^^
  File "/home/minseo/anaconda3/envs/dvg/lib/python3.12/site-packages/fire/core.py", line 143, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/minseo/anaconda3/envs/dvg/lib/python3.12/site-packages/fire/core.py", line 477, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File "/home/minseo/anaconda3/envs/dvg/lib/python3.12/site-packages/fire/core.py", line 693, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/minseo/back_up_3/checker/src/main.py", line 80, in main
    test_recall, test_hit = model_dict[model.lower()](
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/minseo/back_up_3/checker/src/models/LightGCN/LightgcnTrainer.py", line 120, in run_lightgcn
    dataset=dataset,
        ^^^^^^^^^^^^
  File "/home/minseo/back_up_3/checker/src/models/LightGCN/LightgcnTrainer.py", line 106, in train_with_hyper_param
    print(f"test: epoch: {epoch}, recall: {recall}, precision: {precision}, ndcg: {ndcg}")
    ^^^^^
  File "/home/minseo/anaconda3/envs/dvg/lib/python3.12/bdb.py", line 90, in trace_dispatch
    return self.dispatch_line(frame)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/minseo/anaconda3/envs/dvg/lib/python3.12/bdb.py", line 115, in dispatch_line
    if self.quitting: raise BdbQuit
                      ^^^^^^^^^^^^^
bdb.BdbQuit
(Pdb) 
srun: error: node1: task 0: Exited with exit code 1
